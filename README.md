# Bishõgrad

A  Bishõ Autograd engine in python along with a lightweight deep neural-network library! (inspired from [mircrograd by Andrej Karpathy](https://github.com/karpathy/micrograd))

## what's Bishõ?
微小 - Bishõ is Japanese word for 'tiny' since my implementation is very tinyyyyy ^_^ compared to PyTorch/Tensorflow

## what's Hako?
箱 - Hako is Japanese word for 'box' , here Hako signify the neurons in our network ;D

## Installation
- Install via pip:
    ```terminal
    pip install bishograd==0.1.0
    ```

## Example
Wonder how this works? Checkout the [examples](examples). 

## Targets : 
- [x] ReLU Activation function
- [x] Add MLP.training() to automate the whole training code
- [ ] Add ~~Sigmoid~~, LeakyReLU & other activation functions
- [ ] Add loss functions - categorical loss, mean-square loss etc 


## Contribution :
_**This project is open for contribution!**_
- Clone this repository:
```terminal
git clone https://github.com/AK3847/Bishograd.git
```
Primary way to contribute is to either raise an [issue](https://github.com/AK3847/Bishograd/issues) or a [pull request](https://github.com/AK3847/Bishograd/pulls) with proper description and code format. You can contribute to any [Targets](#targets-) or suggest new features as well.

Drop a star if this project helped you in anyway! ( ﾉ ﾟｰﾟ)ﾉ