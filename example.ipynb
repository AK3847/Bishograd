{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the neural network && Bish√µgrad engine\n",
    "from src.bishograd.nn import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample neural-network to solve XOR Logic Gate\n",
    "## The XOR Gate works with 2 inputs: x1 & x2 and produces an output y\n",
    "## when x1 & x2 are opposite (0 & 1 or vice-versa) the outcome 'y' is 1 else 0\n",
    "\n",
    "## all four possible input combinations\n",
    "xor_inputs = [\n",
    "    [1.0,0.0],\n",
    "    [1.0,1.0,],\n",
    "    [0.0,1.0],\n",
    "    [0.0,0.0,]\n",
    "]\n",
    "\n",
    "## outcome for given combinations\n",
    "y_outputs = [1.0,0.0,1.0,0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making our neural network (nn) having input list size as 2 \n",
    "## with one hidden layer of 4 neuron and output layer with 1 neuron\n",
    "\n",
    "nn = MLP(2,[2,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 ---- Loss: 6.5638\n",
      "epoch: 1 ---- Loss: 4.4997\n",
      "epoch: 2 ---- Loss: 2.1617\n",
      "epoch: 3 ---- Loss: 1.1874\n",
      "epoch: 4 ---- Loss: 1.0266\n",
      "epoch: 5 ---- Loss: 0.9916\n",
      "epoch: 6 ---- Loss: 0.9803\n",
      "epoch: 7 ---- Loss: 0.9753\n",
      "epoch: 8 ---- Loss: 0.9721\n",
      "epoch: 9 ---- Loss: 0.9695\n",
      "epoch: 10 ---- Loss: 0.9672\n",
      "epoch: 11 ---- Loss: 0.9650\n",
      "epoch: 12 ---- Loss: 0.9627\n",
      "epoch: 13 ---- Loss: 0.9605\n",
      "epoch: 14 ---- Loss: 0.9582\n",
      "epoch: 15 ---- Loss: 0.9559\n",
      "epoch: 16 ---- Loss: 0.9536\n",
      "epoch: 17 ---- Loss: 0.9513\n",
      "epoch: 18 ---- Loss: 0.9489\n",
      "epoch: 19 ---- Loss: 0.9465\n",
      "epoch: 20 ---- Loss: 0.9441\n",
      "epoch: 21 ---- Loss: 0.9417\n",
      "epoch: 22 ---- Loss: 0.9392\n",
      "epoch: 23 ---- Loss: 0.9367\n",
      "epoch: 24 ---- Loss: 0.9341\n",
      "epoch: 25 ---- Loss: 0.9315\n",
      "epoch: 26 ---- Loss: 0.9289\n",
      "epoch: 27 ---- Loss: 0.9262\n",
      "epoch: 28 ---- Loss: 0.9234\n",
      "epoch: 29 ---- Loss: 0.9207\n",
      "epoch: 30 ---- Loss: 0.9179\n",
      "epoch: 31 ---- Loss: 0.9150\n",
      "epoch: 32 ---- Loss: 0.9121\n",
      "epoch: 33 ---- Loss: 0.9091\n",
      "epoch: 34 ---- Loss: 0.9062\n",
      "epoch: 35 ---- Loss: 0.9031\n",
      "epoch: 36 ---- Loss: 0.9000\n",
      "epoch: 37 ---- Loss: 0.8969\n",
      "epoch: 38 ---- Loss: 0.8938\n",
      "epoch: 39 ---- Loss: 0.8906\n",
      "epoch: 40 ---- Loss: 0.8873\n",
      "epoch: 41 ---- Loss: 0.8841\n",
      "epoch: 42 ---- Loss: 0.8808\n",
      "epoch: 43 ---- Loss: 0.8775\n",
      "epoch: 44 ---- Loss: 0.8741\n",
      "epoch: 45 ---- Loss: 0.8708\n",
      "epoch: 46 ---- Loss: 0.8674\n",
      "epoch: 47 ---- Loss: 0.8641\n",
      "epoch: 48 ---- Loss: 0.8607\n",
      "epoch: 49 ---- Loss: 0.8573\n",
      "epoch: 50 ---- Loss: 0.8540\n",
      "epoch: 51 ---- Loss: 0.8506\n",
      "epoch: 52 ---- Loss: 0.8473\n",
      "epoch: 53 ---- Loss: 0.8440\n",
      "epoch: 54 ---- Loss: 0.8407\n",
      "epoch: 55 ---- Loss: 0.8375\n",
      "epoch: 56 ---- Loss: 0.8343\n",
      "epoch: 57 ---- Loss: 0.8311\n",
      "epoch: 58 ---- Loss: 0.8280\n",
      "epoch: 59 ---- Loss: 0.8250\n",
      "epoch: 60 ---- Loss: 0.8220\n",
      "epoch: 61 ---- Loss: 0.8191\n",
      "epoch: 62 ---- Loss: 0.8162\n",
      "epoch: 63 ---- Loss: 0.8134\n",
      "epoch: 64 ---- Loss: 0.8107\n",
      "epoch: 65 ---- Loss: 0.8081\n",
      "epoch: 66 ---- Loss: 0.8055\n",
      "epoch: 67 ---- Loss: 0.8030\n",
      "epoch: 68 ---- Loss: 0.8006\n",
      "epoch: 69 ---- Loss: 0.7982\n",
      "epoch: 70 ---- Loss: 0.7960\n",
      "epoch: 71 ---- Loss: 0.7938\n",
      "epoch: 72 ---- Loss: 0.7916\n",
      "epoch: 73 ---- Loss: 0.7896\n",
      "epoch: 74 ---- Loss: 0.7876\n",
      "epoch: 75 ---- Loss: 0.7856\n",
      "epoch: 76 ---- Loss: 0.7838\n",
      "epoch: 77 ---- Loss: 0.7820\n",
      "epoch: 78 ---- Loss: 0.7802\n",
      "epoch: 79 ---- Loss: 0.7785\n",
      "epoch: 80 ---- Loss: 0.7769\n",
      "epoch: 81 ---- Loss: 0.7753\n",
      "epoch: 82 ---- Loss: 0.7738\n",
      "epoch: 83 ---- Loss: 0.7723\n",
      "epoch: 84 ---- Loss: 0.7709\n",
      "epoch: 85 ---- Loss: 0.7695\n",
      "epoch: 86 ---- Loss: 0.7681\n",
      "epoch: 87 ---- Loss: 0.7668\n",
      "epoch: 88 ---- Loss: 0.7655\n",
      "epoch: 89 ---- Loss: 0.7643\n",
      "epoch: 90 ---- Loss: 0.7631\n",
      "epoch: 91 ---- Loss: 0.7619\n",
      "epoch: 92 ---- Loss: 0.7607\n",
      "epoch: 93 ---- Loss: 0.7596\n",
      "epoch: 94 ---- Loss: 0.7585\n",
      "epoch: 95 ---- Loss: 0.7574\n",
      "epoch: 96 ---- Loss: 0.7564\n",
      "epoch: 97 ---- Loss: 0.7553\n",
      "epoch: 98 ---- Loss: 0.7543\n",
      "epoch: 99 ---- Loss: 0.7533\n",
      "epoch: 100 ---- Loss: 0.7523\n",
      "epoch: 101 ---- Loss: 0.7514\n",
      "epoch: 102 ---- Loss: 0.7505\n",
      "epoch: 103 ---- Loss: 0.7495\n",
      "epoch: 104 ---- Loss: 0.7486\n",
      "epoch: 105 ---- Loss: 0.7477\n",
      "epoch: 106 ---- Loss: 0.7469\n",
      "epoch: 107 ---- Loss: 0.7460\n",
      "epoch: 108 ---- Loss: 0.7451\n",
      "epoch: 109 ---- Loss: 0.7443\n",
      "epoch: 110 ---- Loss: 0.7435\n",
      "epoch: 111 ---- Loss: 0.7427\n",
      "epoch: 112 ---- Loss: 0.7419\n",
      "epoch: 113 ---- Loss: 0.7411\n",
      "epoch: 114 ---- Loss: 0.7403\n",
      "epoch: 115 ---- Loss: 0.7395\n",
      "epoch: 116 ---- Loss: 0.7388\n",
      "epoch: 117 ---- Loss: 0.7380\n",
      "epoch: 118 ---- Loss: 0.7373\n",
      "epoch: 119 ---- Loss: 0.7366\n",
      "epoch: 120 ---- Loss: 0.7358\n",
      "epoch: 121 ---- Loss: 0.7351\n",
      "epoch: 122 ---- Loss: 0.7344\n",
      "epoch: 123 ---- Loss: 0.7337\n",
      "epoch: 124 ---- Loss: 0.7330\n",
      "epoch: 125 ---- Loss: 0.7323\n",
      "epoch: 126 ---- Loss: 0.7317\n",
      "epoch: 127 ---- Loss: 0.7310\n",
      "epoch: 128 ---- Loss: 0.7303\n",
      "epoch: 129 ---- Loss: 0.7297\n",
      "epoch: 130 ---- Loss: 0.7290\n",
      "epoch: 131 ---- Loss: 0.7284\n",
      "epoch: 132 ---- Loss: 0.7278\n",
      "epoch: 133 ---- Loss: 0.7271\n",
      "epoch: 134 ---- Loss: 0.7265\n",
      "epoch: 135 ---- Loss: 0.7259\n",
      "epoch: 136 ---- Loss: 0.7253\n",
      "epoch: 137 ---- Loss: 0.7247\n",
      "epoch: 138 ---- Loss: 0.7241\n",
      "epoch: 139 ---- Loss: 0.7235\n",
      "epoch: 140 ---- Loss: 0.7229\n",
      "epoch: 141 ---- Loss: 0.7223\n",
      "epoch: 142 ---- Loss: 0.7217\n",
      "epoch: 143 ---- Loss: 0.7212\n",
      "epoch: 144 ---- Loss: 0.7206\n",
      "epoch: 145 ---- Loss: 0.7200\n",
      "epoch: 146 ---- Loss: 0.7195\n",
      "epoch: 147 ---- Loss: 0.7189\n",
      "epoch: 148 ---- Loss: 0.7184\n",
      "epoch: 149 ---- Loss: 0.7178\n",
      "epoch: 150 ---- Loss: 0.7173\n",
      "epoch: 151 ---- Loss: 0.7168\n",
      "epoch: 152 ---- Loss: 0.7162\n",
      "epoch: 153 ---- Loss: 0.7157\n",
      "epoch: 154 ---- Loss: 0.7152\n",
      "epoch: 155 ---- Loss: 0.7146\n",
      "epoch: 156 ---- Loss: 0.7141\n",
      "epoch: 157 ---- Loss: 0.7136\n",
      "epoch: 158 ---- Loss: 0.7131\n",
      "epoch: 159 ---- Loss: 0.7126\n",
      "epoch: 160 ---- Loss: 0.7121\n",
      "epoch: 161 ---- Loss: 0.7116\n",
      "epoch: 162 ---- Loss: 0.7111\n",
      "epoch: 163 ---- Loss: 0.7106\n",
      "epoch: 164 ---- Loss: 0.7101\n",
      "epoch: 165 ---- Loss: 0.7096\n",
      "epoch: 166 ---- Loss: 0.7091\n",
      "epoch: 167 ---- Loss: 0.7086\n",
      "epoch: 168 ---- Loss: 0.7081\n",
      "epoch: 169 ---- Loss: 0.7076\n",
      "epoch: 170 ---- Loss: 0.7072\n",
      "epoch: 171 ---- Loss: 0.7067\n",
      "epoch: 172 ---- Loss: 0.7062\n",
      "epoch: 173 ---- Loss: 0.7057\n",
      "epoch: 174 ---- Loss: 0.7053\n",
      "epoch: 175 ---- Loss: 0.7048\n",
      "epoch: 176 ---- Loss: 0.7043\n",
      "epoch: 177 ---- Loss: 0.7038\n",
      "epoch: 178 ---- Loss: 0.7034\n",
      "epoch: 179 ---- Loss: 0.7029\n",
      "epoch: 180 ---- Loss: 0.7025\n",
      "epoch: 181 ---- Loss: 0.7020\n",
      "epoch: 182 ---- Loss: 0.7015\n",
      "epoch: 183 ---- Loss: 0.7011\n",
      "epoch: 184 ---- Loss: 0.7006\n",
      "epoch: 185 ---- Loss: 0.7002\n",
      "epoch: 186 ---- Loss: 0.6997\n",
      "epoch: 187 ---- Loss: 0.6992\n",
      "epoch: 188 ---- Loss: 0.6988\n",
      "epoch: 189 ---- Loss: 0.6983\n",
      "epoch: 190 ---- Loss: 0.6979\n",
      "epoch: 191 ---- Loss: 0.6974\n",
      "epoch: 192 ---- Loss: 0.6970\n",
      "epoch: 193 ---- Loss: 0.6965\n",
      "epoch: 194 ---- Loss: 0.6960\n",
      "epoch: 195 ---- Loss: 0.6956\n",
      "epoch: 196 ---- Loss: 0.6951\n",
      "epoch: 197 ---- Loss: 0.6947\n",
      "epoch: 198 ---- Loss: 0.6942\n",
      "epoch: 199 ---- Loss: 0.6938\n",
      "epoch: 200 ---- Loss: 0.6933\n",
      "epoch: 201 ---- Loss: 0.6929\n",
      "epoch: 202 ---- Loss: 0.6924\n",
      "epoch: 203 ---- Loss: 0.6919\n",
      "epoch: 204 ---- Loss: 0.6915\n",
      "epoch: 205 ---- Loss: 0.6910\n",
      "epoch: 206 ---- Loss: 0.6906\n",
      "epoch: 207 ---- Loss: 0.6901\n",
      "epoch: 208 ---- Loss: 0.6896\n",
      "epoch: 209 ---- Loss: 0.6892\n",
      "epoch: 210 ---- Loss: 0.6887\n",
      "epoch: 211 ---- Loss: 0.6882\n",
      "epoch: 212 ---- Loss: 0.6877\n",
      "epoch: 213 ---- Loss: 0.6873\n",
      "epoch: 214 ---- Loss: 0.6868\n",
      "epoch: 215 ---- Loss: 0.6863\n",
      "epoch: 216 ---- Loss: 0.6858\n",
      "epoch: 217 ---- Loss: 0.6853\n",
      "epoch: 218 ---- Loss: 0.6849\n",
      "epoch: 219 ---- Loss: 0.6844\n",
      "epoch: 220 ---- Loss: 0.6839\n",
      "epoch: 221 ---- Loss: 0.6834\n",
      "epoch: 222 ---- Loss: 0.6829\n",
      "epoch: 223 ---- Loss: 0.6824\n",
      "epoch: 224 ---- Loss: 0.6819\n",
      "epoch: 225 ---- Loss: 0.6814\n",
      "epoch: 226 ---- Loss: 0.6808\n",
      "epoch: 227 ---- Loss: 0.6803\n",
      "epoch: 228 ---- Loss: 0.6798\n",
      "epoch: 229 ---- Loss: 0.6793\n",
      "epoch: 230 ---- Loss: 0.6787\n",
      "epoch: 231 ---- Loss: 0.6782\n",
      "epoch: 232 ---- Loss: 0.6777\n",
      "epoch: 233 ---- Loss: 0.6771\n",
      "epoch: 234 ---- Loss: 0.6766\n",
      "epoch: 235 ---- Loss: 0.6760\n",
      "epoch: 236 ---- Loss: 0.6754\n",
      "epoch: 237 ---- Loss: 0.6749\n",
      "epoch: 238 ---- Loss: 0.6743\n",
      "epoch: 239 ---- Loss: 0.6737\n",
      "epoch: 240 ---- Loss: 0.6731\n",
      "epoch: 241 ---- Loss: 0.6725\n",
      "epoch: 242 ---- Loss: 0.6719\n",
      "epoch: 243 ---- Loss: 0.6713\n",
      "epoch: 244 ---- Loss: 0.6707\n",
      "epoch: 245 ---- Loss: 0.6700\n",
      "epoch: 246 ---- Loss: 0.6694\n",
      "epoch: 247 ---- Loss: 0.6688\n",
      "epoch: 248 ---- Loss: 0.6681\n",
      "epoch: 249 ---- Loss: 0.6674\n",
      "epoch: 250 ---- Loss: 0.6668\n",
      "epoch: 251 ---- Loss: 0.6661\n",
      "epoch: 252 ---- Loss: 0.6654\n",
      "epoch: 253 ---- Loss: 0.6647\n",
      "epoch: 254 ---- Loss: 0.6640\n",
      "epoch: 255 ---- Loss: 0.6632\n",
      "epoch: 256 ---- Loss: 0.6625\n",
      "epoch: 257 ---- Loss: 0.6617\n",
      "epoch: 258 ---- Loss: 0.6610\n",
      "epoch: 259 ---- Loss: 0.6602\n",
      "epoch: 260 ---- Loss: 0.6594\n",
      "epoch: 261 ---- Loss: 0.6586\n",
      "epoch: 262 ---- Loss: 0.6578\n",
      "epoch: 263 ---- Loss: 0.6569\n",
      "epoch: 264 ---- Loss: 0.6561\n",
      "epoch: 265 ---- Loss: 0.6552\n",
      "epoch: 266 ---- Loss: 0.6543\n",
      "epoch: 267 ---- Loss: 0.6534\n",
      "epoch: 268 ---- Loss: 0.6525\n",
      "epoch: 269 ---- Loss: 0.6515\n",
      "epoch: 270 ---- Loss: 0.6506\n",
      "epoch: 271 ---- Loss: 0.6496\n",
      "epoch: 272 ---- Loss: 0.6486\n",
      "epoch: 273 ---- Loss: 0.6475\n",
      "epoch: 274 ---- Loss: 0.6465\n",
      "epoch: 275 ---- Loss: 0.6454\n",
      "epoch: 276 ---- Loss: 0.6443\n",
      "epoch: 277 ---- Loss: 0.6432\n",
      "epoch: 278 ---- Loss: 0.6420\n",
      "epoch: 279 ---- Loss: 0.6409\n",
      "epoch: 280 ---- Loss: 0.6396\n",
      "epoch: 281 ---- Loss: 0.6384\n",
      "epoch: 282 ---- Loss: 0.6371\n",
      "epoch: 283 ---- Loss: 0.6358\n",
      "epoch: 284 ---- Loss: 0.6345\n",
      "epoch: 285 ---- Loss: 0.6331\n",
      "epoch: 286 ---- Loss: 0.6317\n",
      "epoch: 287 ---- Loss: 0.6302\n",
      "epoch: 288 ---- Loss: 0.6287\n",
      "epoch: 289 ---- Loss: 0.6272\n",
      "epoch: 290 ---- Loss: 0.6256\n",
      "epoch: 291 ---- Loss: 0.6240\n",
      "epoch: 292 ---- Loss: 0.6223\n",
      "epoch: 293 ---- Loss: 0.6206\n",
      "epoch: 294 ---- Loss: 0.6188\n",
      "epoch: 295 ---- Loss: 0.6169\n",
      "epoch: 296 ---- Loss: 0.6150\n",
      "epoch: 297 ---- Loss: 0.6131\n",
      "epoch: 298 ---- Loss: 0.6110\n",
      "epoch: 299 ---- Loss: 0.6089\n",
      "epoch: 300 ---- Loss: 0.6067\n",
      "epoch: 301 ---- Loss: 0.6045\n",
      "epoch: 302 ---- Loss: 0.6022\n",
      "epoch: 303 ---- Loss: 0.5998\n",
      "epoch: 304 ---- Loss: 0.5973\n",
      "epoch: 305 ---- Loss: 0.5947\n",
      "epoch: 306 ---- Loss: 0.5920\n",
      "epoch: 307 ---- Loss: 0.5892\n",
      "epoch: 308 ---- Loss: 0.5863\n",
      "epoch: 309 ---- Loss: 0.5833\n",
      "epoch: 310 ---- Loss: 0.5801\n",
      "epoch: 311 ---- Loss: 0.5769\n",
      "epoch: 312 ---- Loss: 0.5735\n",
      "epoch: 313 ---- Loss: 0.5699\n",
      "epoch: 314 ---- Loss: 0.5662\n",
      "epoch: 315 ---- Loss: 0.5624\n",
      "epoch: 316 ---- Loss: 0.5584\n",
      "epoch: 317 ---- Loss: 0.5542\n",
      "epoch: 318 ---- Loss: 0.5498\n",
      "epoch: 319 ---- Loss: 0.5453\n",
      "epoch: 320 ---- Loss: 0.5405\n",
      "epoch: 321 ---- Loss: 0.5355\n",
      "epoch: 322 ---- Loss: 0.5303\n",
      "epoch: 323 ---- Loss: 0.5248\n",
      "epoch: 324 ---- Loss: 0.5191\n",
      "epoch: 325 ---- Loss: 0.5132\n",
      "epoch: 326 ---- Loss: 0.5069\n",
      "epoch: 327 ---- Loss: 0.5004\n",
      "epoch: 328 ---- Loss: 0.4936\n",
      "epoch: 329 ---- Loss: 0.4864\n",
      "epoch: 330 ---- Loss: 0.4790\n",
      "epoch: 331 ---- Loss: 0.4712\n",
      "epoch: 332 ---- Loss: 0.4630\n",
      "epoch: 333 ---- Loss: 0.4546\n",
      "epoch: 334 ---- Loss: 0.4457\n",
      "epoch: 335 ---- Loss: 0.4366\n",
      "epoch: 336 ---- Loss: 0.4270\n",
      "epoch: 337 ---- Loss: 0.4171\n",
      "epoch: 338 ---- Loss: 0.4069\n",
      "epoch: 339 ---- Loss: 0.3963\n",
      "epoch: 340 ---- Loss: 0.3855\n",
      "epoch: 341 ---- Loss: 0.3743\n",
      "epoch: 342 ---- Loss: 0.3629\n",
      "epoch: 343 ---- Loss: 0.3513\n",
      "epoch: 344 ---- Loss: 0.3394\n",
      "epoch: 345 ---- Loss: 0.3274\n",
      "epoch: 346 ---- Loss: 0.3154\n",
      "epoch: 347 ---- Loss: 0.3033\n",
      "epoch: 348 ---- Loss: 0.2912\n",
      "epoch: 349 ---- Loss: 0.2791\n",
      "epoch: 350 ---- Loss: 0.2672\n",
      "epoch: 351 ---- Loss: 0.2555\n",
      "epoch: 352 ---- Loss: 0.2440\n",
      "epoch: 353 ---- Loss: 0.2328\n",
      "epoch: 354 ---- Loss: 0.2219\n",
      "epoch: 355 ---- Loss: 0.2113\n",
      "epoch: 356 ---- Loss: 0.2012\n",
      "epoch: 357 ---- Loss: 0.1915\n",
      "epoch: 358 ---- Loss: 0.1822\n",
      "epoch: 359 ---- Loss: 0.1733\n",
      "epoch: 360 ---- Loss: 0.1649\n",
      "epoch: 361 ---- Loss: 0.1569\n",
      "epoch: 362 ---- Loss: 0.1493\n",
      "epoch: 363 ---- Loss: 0.1422\n",
      "epoch: 364 ---- Loss: 0.1355\n",
      "epoch: 365 ---- Loss: 0.1291\n",
      "epoch: 366 ---- Loss: 0.1232\n",
      "epoch: 367 ---- Loss: 0.1176\n",
      "epoch: 368 ---- Loss: 0.1123\n",
      "epoch: 369 ---- Loss: 0.1074\n",
      "epoch: 370 ---- Loss: 0.1027\n",
      "epoch: 371 ---- Loss: 0.0983\n",
      "epoch: 372 ---- Loss: 0.0942\n",
      "epoch: 373 ---- Loss: 0.0904\n",
      "epoch: 374 ---- Loss: 0.0867\n",
      "epoch: 375 ---- Loss: 0.0833\n",
      "epoch: 376 ---- Loss: 0.0801\n",
      "epoch: 377 ---- Loss: 0.0770\n",
      "epoch: 378 ---- Loss: 0.0742\n",
      "epoch: 379 ---- Loss: 0.0714\n",
      "epoch: 380 ---- Loss: 0.0689\n",
      "epoch: 381 ---- Loss: 0.0665\n",
      "epoch: 382 ---- Loss: 0.0642\n",
      "epoch: 383 ---- Loss: 0.0620\n",
      "epoch: 384 ---- Loss: 0.0599\n",
      "epoch: 385 ---- Loss: 0.0580\n",
      "epoch: 386 ---- Loss: 0.0561\n",
      "epoch: 387 ---- Loss: 0.0544\n",
      "epoch: 388 ---- Loss: 0.0527\n",
      "epoch: 389 ---- Loss: 0.0511\n",
      "epoch: 390 ---- Loss: 0.0496\n",
      "epoch: 391 ---- Loss: 0.0482\n",
      "epoch: 392 ---- Loss: 0.0468\n",
      "epoch: 393 ---- Loss: 0.0455\n",
      "epoch: 394 ---- Loss: 0.0442\n",
      "epoch: 395 ---- Loss: 0.0430\n",
      "epoch: 396 ---- Loss: 0.0419\n",
      "epoch: 397 ---- Loss: 0.0408\n",
      "epoch: 398 ---- Loss: 0.0397\n",
      "epoch: 399 ---- Loss: 0.0387\n",
      "epoch: 400 ---- Loss: 0.0378\n",
      "epoch: 401 ---- Loss: 0.0368\n",
      "epoch: 402 ---- Loss: 0.0359\n",
      "epoch: 403 ---- Loss: 0.0351\n",
      "epoch: 404 ---- Loss: 0.0343\n",
      "epoch: 405 ---- Loss: 0.0335\n",
      "epoch: 406 ---- Loss: 0.0327\n",
      "epoch: 407 ---- Loss: 0.0320\n",
      "epoch: 408 ---- Loss: 0.0313\n",
      "epoch: 409 ---- Loss: 0.0306\n",
      "epoch: 410 ---- Loss: 0.0300\n",
      "epoch: 411 ---- Loss: 0.0293\n",
      "epoch: 412 ---- Loss: 0.0287\n",
      "epoch: 413 ---- Loss: 0.0281\n",
      "epoch: 414 ---- Loss: 0.0276\n",
      "epoch: 415 ---- Loss: 0.0270\n",
      "epoch: 416 ---- Loss: 0.0265\n",
      "epoch: 417 ---- Loss: 0.0260\n",
      "epoch: 418 ---- Loss: 0.0255\n",
      "epoch: 419 ---- Loss: 0.0250\n",
      "epoch: 420 ---- Loss: 0.0246\n",
      "epoch: 421 ---- Loss: 0.0241\n",
      "epoch: 422 ---- Loss: 0.0237\n",
      "epoch: 423 ---- Loss: 0.0233\n",
      "epoch: 424 ---- Loss: 0.0229\n",
      "epoch: 425 ---- Loss: 0.0225\n",
      "epoch: 426 ---- Loss: 0.0221\n",
      "epoch: 427 ---- Loss: 0.0217\n",
      "epoch: 428 ---- Loss: 0.0213\n",
      "epoch: 429 ---- Loss: 0.0210\n",
      "epoch: 430 ---- Loss: 0.0206\n",
      "epoch: 431 ---- Loss: 0.0203\n",
      "epoch: 432 ---- Loss: 0.0200\n",
      "epoch: 433 ---- Loss: 0.0197\n",
      "epoch: 434 ---- Loss: 0.0194\n",
      "epoch: 435 ---- Loss: 0.0191\n",
      "epoch: 436 ---- Loss: 0.0188\n",
      "epoch: 437 ---- Loss: 0.0185\n",
      "epoch: 438 ---- Loss: 0.0182\n",
      "epoch: 439 ---- Loss: 0.0180\n",
      "epoch: 440 ---- Loss: 0.0177\n",
      "epoch: 441 ---- Loss: 0.0175\n",
      "epoch: 442 ---- Loss: 0.0172\n",
      "epoch: 443 ---- Loss: 0.0170\n",
      "epoch: 444 ---- Loss: 0.0167\n",
      "epoch: 445 ---- Loss: 0.0165\n",
      "epoch: 446 ---- Loss: 0.0163\n",
      "epoch: 447 ---- Loss: 0.0161\n",
      "epoch: 448 ---- Loss: 0.0158\n",
      "epoch: 449 ---- Loss: 0.0156\n",
      "epoch: 450 ---- Loss: 0.0154\n",
      "epoch: 451 ---- Loss: 0.0152\n",
      "epoch: 452 ---- Loss: 0.0150\n",
      "epoch: 453 ---- Loss: 0.0148\n",
      "epoch: 454 ---- Loss: 0.0147\n",
      "epoch: 455 ---- Loss: 0.0145\n",
      "epoch: 456 ---- Loss: 0.0143\n",
      "epoch: 457 ---- Loss: 0.0141\n",
      "epoch: 458 ---- Loss: 0.0140\n",
      "epoch: 459 ---- Loss: 0.0138\n",
      "epoch: 460 ---- Loss: 0.0136\n",
      "epoch: 461 ---- Loss: 0.0135\n",
      "epoch: 462 ---- Loss: 0.0133\n",
      "epoch: 463 ---- Loss: 0.0132\n",
      "epoch: 464 ---- Loss: 0.0130\n",
      "epoch: 465 ---- Loss: 0.0129\n",
      "epoch: 466 ---- Loss: 0.0127\n",
      "epoch: 467 ---- Loss: 0.0126\n",
      "epoch: 468 ---- Loss: 0.0124\n",
      "epoch: 469 ---- Loss: 0.0123\n",
      "epoch: 470 ---- Loss: 0.0122\n",
      "epoch: 471 ---- Loss: 0.0120\n",
      "epoch: 472 ---- Loss: 0.0119\n",
      "epoch: 473 ---- Loss: 0.0118\n",
      "epoch: 474 ---- Loss: 0.0117\n",
      "epoch: 475 ---- Loss: 0.0115\n",
      "epoch: 476 ---- Loss: 0.0114\n",
      "epoch: 477 ---- Loss: 0.0113\n",
      "epoch: 478 ---- Loss: 0.0112\n",
      "epoch: 479 ---- Loss: 0.0111\n",
      "epoch: 480 ---- Loss: 0.0110\n",
      "epoch: 481 ---- Loss: 0.0109\n",
      "epoch: 482 ---- Loss: 0.0108\n",
      "epoch: 483 ---- Loss: 0.0107\n",
      "epoch: 484 ---- Loss: 0.0106\n",
      "epoch: 485 ---- Loss: 0.0105\n",
      "epoch: 486 ---- Loss: 0.0104\n",
      "epoch: 487 ---- Loss: 0.0103\n",
      "epoch: 488 ---- Loss: 0.0102\n",
      "epoch: 489 ---- Loss: 0.0101\n",
      "epoch: 490 ---- Loss: 0.0100\n",
      "epoch: 491 ---- Loss: 0.0099\n",
      "epoch: 492 ---- Loss: 0.0098\n",
      "epoch: 493 ---- Loss: 0.0097\n",
      "epoch: 494 ---- Loss: 0.0096\n",
      "epoch: 495 ---- Loss: 0.0095\n",
      "epoch: 496 ---- Loss: 0.0095\n",
      "epoch: 497 ---- Loss: 0.0094\n",
      "epoch: 498 ---- Loss: 0.0093\n",
      "epoch: 499 ---- Loss: 0.0092\n"
     ]
    }
   ],
   "source": [
    "## training our neural network for 50 epochs\n",
    "learning_rate = 0.05 ## learning_rate for changing the gradients\n",
    "epochs = 500 ## number of epochs for training the neural network\n",
    "for k in range(epochs):\n",
    "    y_predict = [nn(x) for x in xor_inputs] ## predicting the outcome for each input and storing it in a list\n",
    "    loss = sum((yout - ygt) ** 2 for yout, ygt in zip(y_predict, y_outputs)) ## calculating the loss (Square-sum error here)\n",
    "\n",
    "    nn.zero_grad() ## setting all the gradients to zero\n",
    "    loss.backward() ## backward propagation\n",
    "    for param in nn.parameters():\n",
    "        param.data -= (learning_rate * param.grad)  ## adjusting our weights using the learning_rate & gradient\n",
    "\n",
    "    print(f'epoch: {k} ---- Loss: {loss.data:.4f}') ## printing the epoch number and loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0, 1] --- Prediction: 0.93 --- Correct Output: 1\n",
      "Input: [1, 0] --- Prediction: 0.94 --- Correct Output: 1\n",
      "Input: [0, 0] --- Prediction: 0.01 --- Correct Output: 0\n",
      "Input: [1, 1] --- Prediction: 0.01 --- Correct Output: 0\n"
     ]
    }
   ],
   "source": [
    "## testing our model \n",
    "x_test =[\n",
    "     [0,1],\n",
    "     [1,0],\n",
    "     [0,0],\n",
    "     [1,1]\n",
    "]\n",
    "for i in x_test:\n",
    "     y_test = nn(x=i)\n",
    "     print(f'Input: {i} --- Prediction: {y_test.data:.2f} --- Correct Output: {i[0]^i[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
